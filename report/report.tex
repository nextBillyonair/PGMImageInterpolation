\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usepackage{color}
\usepackage{array}
\usepackage{dsfont}
\usepackage{multirow, graphicx}
 \usepackage{float}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\usepackage{caption}
\usepackage{subcaption}
\urlstyle{same}

\title{The Effect of Image Interpolation on the Generalization of Generative and Discriminative Models}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  William Watson \\
  The Johns Hopkins University\\
  \texttt{billwatson@jhu.edu} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  In statistical classification, modeling can be approached in the \textit{generative}
  or \textit{discriminative} way. In computer vision, rescaling
  techniques known as \textit{image interpolation} are analogous to
  dimensionality reduction. This report explores the generalization and tractability of
  \textit{generative-discriminative pairs} when interpolating images to smaller
  sizes.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Minimum length: 4 pages. Maximum length: 8 pages !!!!!                   %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}
\label{sec:intro}
% something about how this might be interesting
Probabilistic graphical models have many applications in computer science,
ranging from image segmentation to named entity recognition. However, these
models can become unwieldy when the feature space $\mathbf{x}$ grows. In the
case of images, as the number of pixels increases, the
total number of raw features grows exponentially. For square rasterized images,
the total number of raw features is $3 \times size^2$.

This leads to issues of tractability for both parameter estimation and
inference. This paper concerns itself with using image interpolation as
a dimensionality reduction technique to make learning tractable. We vary the
dimensionality to examine the generalization ability of the models and the
efficiency of parameter estimation. We describe the data in Section \ref{sec:data},
experimental procedure in Section \ref{sec:task}, and image interpolation
(Section \ref{sec:img-interpolation}).

We experiment with two modeling approaches: \textit{generative}
and \textit{discriminative} learning. Both approaches are outlined in Section
\ref{sec:classification}. The models are formally described in Sections
\ref{sec:baseline-models} and \ref{sec:general-models}. Our results
are presented in Sections \ref{sec:results-baseline} and \ref{sec:results-general}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% steal from proposal
\section{Statistical Classification}
\label{sec:classification}
Classification is the task of assigning a label $y$ to a set of observed
features $\mathbf{x}$. Yet the way we approach modeling these relationships
can be broken down into either the \textit{generative} or the
\textit{discriminative} approach.

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \scalebox{0.8}{
        \tikz{ %
          \node[latent] (y) {$y$} ; %
          \node[obs, below=of y] (x) {$\mathbf{x}$} ; %
          \edge {y} {x} ; %
        }}
        \caption*{Generative}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \scalebox{0.8}{
        \tikz{ %
          \node[obs] (x) {$\mathbf{x}$} ; %
          \node[latent, below=of x] (y) {$y$} ; %
          \node[factor, below=of x] (factor-node) {} ; %
          \factoredge[-] {x} {factor-node} {y} ; %
        }}
        \caption*{Discriminative}
    \end{subfigure}
\end{figure*}

The generative approach models the joint distribution $p(y, \mathbf{x})$,
and can assign labels through Bayes rule \cite{NgJordan}.
\begin{equation}
    \hat{y} \; = \; \argmax_y \; p(y, \mathbf{x}) \; = \; \argmax_y \; p(y) \cdot p(\mathbf{x} | y) \\
\end{equation}
Generative models are attractive in that distributions are learned over the
feature space for individual classes. However, an important limitation
mentioned by McCallum et al. is that modeling a distribution
\textit{per feature} quickly becomes intractable as the dimensionality of our
observed variables $\mathbf{x}$ grows \cite{McCallumCRF}. While simple models
can mitigate these issues by assuming independence among the features,
allowing complex dependencies between inputs may result in
increased performance.

An alternative approach is to model the conditional probabilities directly,
ignoring the feature distributions. This is the discriminative approach,
and is sometimes referred to as a \textit{distribution-free classifier}.
By ignoring the feature distributions, parameters are learned only on the
conditional likelihood $p(y | \mathbf{x})$, resulting in a compact model
that can handle large feature spaces, as dependencies
are ignored \cite{McCallumCRF}.

To compare generative and discriminative modeling, one can experiment
with pairs of classifiers that can be considered analogous to each other.
More formally, a \textit{generative-discriminative pair} is a parametric
family of probabilistic models that can either be fit to maximize the joint
probability $p(y, \mathbf{x})$ or the conditional
likelihood $p(y | \mathbf{x})$ \cite{NgJordan}. The simplest pairing is
the Naive Bayes classifier (generative) and Logistic Regression
(discriminative). This paradigm can be extended to sequential and general
models to form more pairs. We discuss our experimental pairs
in Sections \ref{sec:baseline-models} and \ref{sec:general-models}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Image Interpolation}
\label{sec:img-interpolation}
The most common problem encountered in machine learning is the
\textit{curse of dimensionality}. Essentially, as the feature space
$\mathbf{x}$ grows in size, we require a larger number of parameters and samples
to effectively learn. This can quickly become intractable in complex models, and
reasonably difficult in simple ones. The growth
in features as image size increases is apparent in Figure \ref{fig:feature-size}.
One method to reduce the number of
features for a data set is Principle Component Analysis, which projects
data to a smaller dimension with components that maximize the variance. However,
interpretability is lost, and the structure of the painting is unintelligible.

\begin{figure}[h!]
  \centering
  \caption{As the image size increases, the problem quickly becomes intractable}
  \label{fig:feature-size}
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[height=3.25cm]{feature_graph.png}
    \caption{Growth in the Number of Features}
    \label{fig:feature-graph}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[height=3.25cm]{param_graph.png}
    \caption{Growth in the Number of Model Parameters}
    \label{fig:param-graph}
  \end{subfigure}
\end{figure}

Image interpolation offers a convenient way to downsample images according to
their local neighborhood, aggregating otherwise noisy estimates into a single
value. This may be beneficial for classification tasks, as interpolation is
analogous to dimensionality reduction while preserving the structure of
the original content.
Interpolation can be used to both decimate and expand the image. Several
methods exists for this task, including linear, bicubic, nearest neighbor, and
\textit{area relation} \cite{opencv}.

% downsize this
\begin{figure}[h!]
  \centering
  \caption{Results of Interpolating a 249,000 pixel image to a 4,096 pixel image.}
  \vspace{-5pt}
  \label{fig:interpolation}
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[height=3.5cm]{ori_img.png}
    \vspace{-10pt}
    \caption{Original Image ($498 \times 500$)}
    \label{fig:ori-image}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[height=3.5cm]{decimated_img.png}
    \vspace{-10pt}
    \caption{Decimated Image ($64 \times 64$)}
    \label{fig:reduced-img}
  \end{subfigure}

  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[height=2.45cm]{ori_pixel_plot.png}
    \caption{Pixel Plot for the Original Image}
    \label{fig:ori-pixel-plot}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[height=2.45cm]{decimated_pixel_plot.png}
    \caption{Pixel Plot for the Decimated Image}
    \label{fig:reduced-pixel-plot}
  \end{subfigure}
\end{figure}

The preferred method for image decimation, or downsampling, is
resampling using pixel area relation as it gives moire'-free results. This can be
seen in Figure \ref{fig:interpolation} that the feature space of the pixels in
relation to their original values is comparable, albeit with information loss.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Data}
\label{sec:data}
The data is drawn from a subset of the Rijkmuseum Dataset \cite{Rijksmuseum}.
In order to have enough samples for a challenging classification task, we
elected to perform type categorization. We used the top $4$ art types: paintings,
photos, drawings, and prints. Each category is limited to the size of the
smallest set, which is $2,269$ photos. Hence the dataset has $9,076$ images
split evenly between the $4$ classes. For training purposes, $10$\% of the
data is used as the test set ($8,168$ training, $908$ test).

 \begin{figure}[h!]
   \caption{Class Examples}
   \vspace{-5pt}
   \label{fig:examples}
   \begin{subfigure}[b]{0.23\textwidth}
     \centering
     \includegraphics[height=2.25cm]{sample_painting.png}
     \caption{Sample Painting}
     \label{fig:painting}
   \end{subfigure}
   ~
   \begin{subfigure}[b]{0.23\textwidth}
     \centering
     \includegraphics[height=2.25cm]{sample_photo.png}
     \caption{Sample Photo}
     \label{fig:photo}
   \end{subfigure}
   ~
   \begin{subfigure}[b]{0.23\textwidth}
     \centering
     \includegraphics[height=2.25cm]{sample_drawing.png}
     \caption{Sample Drawing}
     \label{fig:drawing}
   \end{subfigure}
   ~
   \begin{subfigure}[b]{0.23\textwidth}
     \centering
     \includegraphics[height=2.25cm]{sample_print.png}
     \caption{Sample Print}
     \label{fig:print}
   \end{subfigure}
 \end{figure}

From Figure \ref{fig:examples}, paintings are vastly different
from the other classes, while photos and prints may be difficult to distinguish
between. Photos and prints share a similar hue and tone, and drawings do not
always have the vibrant colors found in paintings. Hence, the 4-way
classification task should be difficult to learn, allowing ample room to
experiment with different models.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Baseline Models}
\label{sec:baseline-models}
\subsection{Gaussian Naive Bayes}
\label{sec:NB}
Naive Bayes is a simple generative classifier based on applying Bayes' theorem
to extract the conditional likelihood $p(y \mid \mathbf{x})$. The fundamental
assumption for the features is "naive", assuming that every feature
is conditionally independent given then class label $y$ for $K$ features.
We can exploit the conditional independence assumption and that
$p(\mathbf{x})$ is constant, hence our formulation for the joint
distribution is:
\begin{equation}
    p(y, \mathbf{x}) \; = \; p(y) \cdot \prod_{k=1}^K p(x_k \mid y)
\end{equation}
Our task is to find the class label $y$ that maximizes the posterior conditional
likelihood:
\begin{equation}
  \begin{aligned}
    \hat{y} \; = \; & \argmax_y \; p(y | \mathbf{x})
    = \;  \argmax_y \; \frac{p(y) \cdot \prod_{k=1}^K p(x_k \mid y)}{p(\mathbf{x})} \\
    = \; & \argmax_y \; p(y) \cdot \prod_{k=1}^K p(x_k \mid y)
  \end{aligned}
\end{equation}
Here, $p(y)$ is the frequency of class $y$ in the training set.
For our purposes we assume the features are normally distributed per class.
This is beneficial for learning, as we learn class-wise means and variances,
in contrast to the discrete case where each variable can take on $256$ states.
Hence, our conditional feature likelihood is:
\begin{equation}
  p(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma^2_y}\right)
\end{equation}

To learn a Gaussian Naive Bayes model, we use Maximum Likelihood Estimation (MLE) for
the parameters $\mu_{kc}$ and $\sigma_{kc}$ per feature $k$ and class $c$, and class weights
$\theta_c$. The MLE equations for $N$ samples are:
\begin{equation}
  \begin{aligned}
    \hat{\theta}_c \; = \; & \frac{1}{N} \sum_{i=1}^N \mathds{1} \left \{ y_i = y_c \right \} = \frac{N_c}{N} \\
    \hat{\mu}_{kc} \; = \; & \frac{1}{N_c} \sum_{i=1}^N \mathds{1} \left \{ y_i = y_c \right \} \cdot x_{ik} \\
    \hat{\sigma}_{kc} \; = \; & \frac{1}{N_c} \sum_{i=1}^N \mathds{1} \left \{ y_i = y_c \right \} \cdot \left( x_{ik} - \mu_{kc} \right)^2 \\
  \end{aligned}
\end{equation}
Here, $N_c = \sum_{i=1}^N \mathds{1} \left \{ y_i = y_c \right \} $, or the number
of data points belonging to class $c$ \cite{murphy}.
\subsection{Multinomial Logistic Regression}
\label{sec:LR}
Logistic Regression is the discriminative analog to the Naive Bayes
classifier, sometimes referred to as the \textit{maximum-entropy classifier}
or a normalized \textit{log-linear model}. As a discriminative model,
we can optimize directly for the conditional likelihood $p(y \mid \mathbf{x})$,
with $Z(\mathbf{x})$ as the normalizing partition function.
\begin{equation}
  p(y \mid \mathbf{x}) = \frac{1}{Z(\mathbf{x})} \exp \Bigg\{ \sum_{j=1}^K \theta_k f_k\left( y, \mathbf{x} \right) \Bigg\}
\end{equation}

McCallum et al. formulates the Logistic Regression model, parameterized by
weights $\theta \in \mathbb{R}^K$, in which a single set of weights is shared
across all the classes \cite{McCallumCRF}. This is achieved through a set of
feature functions that are nonzero for a single class:
\begin{equation}
  \begin{aligned}
    f_{y', j} \left( y, \mathbf{x} \right) \; = \; & \mathds{1} \{ y' = y \} \cdot x_j \\
    f_{y'} \left(y, \mathbf{x}\right) \; = \; & \mathds{1} \{ y' = y \} \\
  \end{aligned}
\end{equation}
This notation, while not standard, mirrors our later formulation of Conditional
Random Fields in Section \ref{sec:CRF}.

For parameter estimation, we opted to use scikit-learn's implementation
of the L-BFGS algorithm, which approximates the Broyden-Fletcher-Goldfarb-Shanno
algorithm \cite{scikit-learn}. This is a quasi-Newton optimization method that approximates the
Hessian matrix to reduce memory usage. Scikit-learn optimizes the L2 penalized
multinomial loss \cite{scikit-learn} \cite{murphy}. Here, $y_i$ is the true
label and $y_c$ could be any label:
\begin{equation}
  \min_{\theta} \; \frac{1}{2} \cdot \theta^T \theta - \sum_{i=1}^N \Bigg[ \sum_{c=1}^C \Bigg[ \mathds{1} \{ y_i = y_c \} \cdot \sum_{k=1}^K \theta_k f_k\left( y_i, \mathbf{x}_i \right) \Bigg] - \log Z(\mathbf{x}_i)  \Bigg]
\end{equation}

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Generative: Naive Bayes}
        \scalebox{0.6}{
        \tikz{ %
          \centering
          \node[latent] (y) {$y$} ; %
          \node[obs, below=of y, xshift=-1cm] (x1) {$x_1$} ; %
          \node[obs, below=of y] (x2) {$x_2$} ; %
          \node[obs, below=of y, xshift=1cm] (x3) {$x_3$} ; %
          \edge {y} {x1, x2, x3} ; %
        }}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Generative: General Bayesian Networks}
        \scalebox{0.6}{
        \tikz{ %
          \centering
          \node[latent] (x1) {$x_1$} ; %
          \node[latent, right=of x1] (x2) {$x_2$};
          \node[latent, right=of x2] (x3) {$x_3$};
          \node[obs, below=of x1] (x4) {$x_4$} ; %
          \node[obs, right=of x4] (x5) {$x_5$} ; %
          \node[obs, right=of x5] (x6) {$x_6$} ; %

          \edge {x1} {x4};
          \edge {x4} {x5};
          \edge {x2} {x5,x6,x3};
          \edge {x3} {x6};
          \path[->] (x1) edge[bend left] node [right] {} (x3);
        }}
    \end{subfigure}

    \vspace{10pt}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Discriminative: Logistic Regression}
        \scalebox{0.6}{
        \tikz{ %
          \centering
          \node[latent] (y) {$y$} ; %
          \node[obs, below=of y, xshift=-1cm] (x1) {$x_1$} ; %
          \node[obs, below=of y] (x2) {$x_2$} ; %
          \node[obs, below=of y, xshift=1cm] (x3) {$x_3$} ; %
          \node[factor, below=of y, xshift=-0.5cm] (factor-node-1) {} ; %
          \node[factor, below=of y] (factor-node-2) {} ; %
          \node[factor, below=of y, xshift=0.5cm] (factor-node-3) {} ; %
          \factoredge[-] {y} {factor-node-1} {x1}; %
          \factoredge[-] {y} {factor-node-2} {x2}; %
          \factoredge[-] {y} {factor-node-3} {x3}; %
        }}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Discriminative: Conditional Random Fields}
        \scalebox{0.6}{
        \tikz{ %
          \centering
          \node[latent] (x1) {$x_1$} ; %
          \node[latent, right=of x1, yshift=-0.7cm] (x2) {$x_2$};
          \node[latent, right=of x2, yshift=0.7cm] (x3) {$x_3$};
          \node[obs, below=of x1] (x4) {$x_4$} ; %
          \node[obs, right=of x4, xshift=0.5cm] (x5) {$x_5$} ; %
          \node[obs, below=of x3] (x6) {$x_6$} ; %
          \node[factor, below=of x1] (f1) {};
          \node[factor, above=of x2, yshift=-0.15cm] (f2) {};
          \node[factor, right=of x4, xshift=.5cm] (f3) {};
          \node[factor, below=of x3, yshift=0.15cm] (f4) {};

          \factoredge[-] {x1} {f1} {x4}; %
          \factoredge[-] {x1} {f2} {x3, x2}; %
          \factoredge[-] {x4} {f3} {x2, x5}; %
          \factoredge[-] {x3} {f4} {x2, x6}; %
        }}
    \end{subfigure}
\end{figure}

\section{General Models}
\label{sec:general-models}
\subsection{Hybrid Bayesian Networks}
\label{sec:BN}
General Bayesian Networks (GBNs) are directed graphical models that are
comprised of a set of nodes and edges. The nodes are random variables,
and they interrelate through directed edges. This allows us to
encode complex relations between the features $\mathbf{x}$ that are not
captured by the Naive Bayes classifier. By exploiting these relationships,
the joint distribution can be decomposed into factors conditioned on the node's
parents \cite{murphy}.
\begin{equation}
  p(\mathbf{x}) \; = \; \prod_{k=1}^K p(x_k \mid pa(x_k))
\end{equation}

Since the labels are discrete and the raw features are continuous, we use
Hybrid Bayesian Networks, otherwise known as
\textit{Conditional Linear Gaussian (CLG)} models if
every discrete variable has only discrete parents and every continuous variable
is represented as a CLG Conditional Probability Distribution (CPD) \cite{koller}.
Continuous nodes make use of the multivariate gaussian distribution, parameterized
by a mean vector $\mathbf{\mu}$ and a symmetric square covariance matrix $\Sigma$.
\begin{equation}
  p(\mathbf{x}; \mathbf{\mu}, \Sigma) \; = \; \frac{1}{(2 \pi)^{n/2} \mid \Sigma \mid^{1/2}} \exp \Big\{ -\frac{1}{2} (\mathbf{x} - \mathbf{\mu})^T \Sigma^{-1} (\mathbf{x} - \mathbf{\mu})  \Big\}
\end{equation}
The conditional linear Gaussian CPD for a continuous variable $x_j$, discrete
parents $\mathbf{U} = \{U_1, \hdots, U_M\}$, continuous parents
$\mathbf{C} = \{C_1, \hdots, C_K\}$ is, for every value
$\mathbf{u} \in Val(\mathbf{U})$ with $k+1$ coefficients
$a_{\mathbf{u},0}, \hdots, a_{\mathbf{u}, k}$ and variance
$\sigma^2_{\mathbf{u}}$:
\begin{equation}
  p(x_j \mid \mathbf{u}, \mathbf{c}) \; = \; \mathcal{N} \Bigg(a_{\mathbf{u},0} + \sum_{k=1}^K a_{\mathbf{u},k} \cdot c_k \; ; \; \sigma^2_{\mathbf{u}}  \Bigg)
\end{equation}
The log likelihood function $\ell$ for a continuous node $X_i$ is:
\begin{equation}
  \ell \; = \; \log \mathcal{L} \; = \; \sum_{i=1}^{N} \Bigg\{ -\frac{1}{2} \log (2\pi \sigma^2_{\mathbf{u}}) - \frac{1}{2} \cdot \frac{1}{\sigma^2_{\mathbf{u}}} \Bigg[ a_{\mathbf{u},0} + \sum_{k=1}^K a_{\mathbf{u},k} \cdot c_k^i - x_j^i \Bigg]^2 \Bigg\}
\end{equation}
The MLE parameters are obtained by ordinary linear regression of each node $x_j$
on its continuous parents $c \in \mathbf{C}$, for each class value
$\mathbf{u} \in \{0, 1, 2, 3\}$ of its discrete parent \cite{koller}.

In contrast, our discrete node is the class label $y$, which is just the frequency.
The maximum likelihood estimate is:
\begin{equation}
  \hat{\theta}_{y_c} = \frac{ \sum_{i=1}^{N} \mathds{1} \{ y^{i} = y_c \} }{ \sum_{i=1}^{N} \sum_{y_k} \mathds{1} \{y^i = y_k\} } = \frac{\sum_{i=1}^{N} \mathds{1} \{ y^{i} = y_c \}}{N}
\end{equation}
Here, $y_c$ is the class being estimated, $y^i$ is the label for the $i$-th sample,
and $y_k$ are the other classes. $N$ is the number of samples.

We elected to use a hybrid model as the discrete case led to
\textit{data fragmentation}, and not every state configuration can be seen
in the training set.

Inference is performed by averaging likelihood weighting simulations using
$n=500$ random samples. For discrete values, the prediction is the highest
conditional probability. The predicted value of a continuous value is the
expected value of the conditional distribution.

\begin{figure}
  \centering
  \scalebox{0.7}{
  \tikz{ %
    \centering
    \node (R) {$R_{i,j}$} ; %
    \node[right=of R] (G) {$G_{i,j}$};
    \node[right=of G] (B) {$B_{i,j}$};
    \node[below=of R] (R2) {$R_{i+1, j}$} ; %
    \node[below=of G] (G2) {$G_{i+1, j}$} ; %
    \node[below=of B] (B2) {$B_{i+1, j}$} ; %
    \node[right=of B] (R3) {$R_{i, j+1}$} ; %
    \node[right=of R3] (G3) {$G_{i, j+1}$} ; %
    \node[right=of G3] (B3) {$B_{i, j+1}$} ; %

    \edge {R} {G};
    \path[->] (R) edge[bend left] node [right] {} (B);
    \edge {G} {B};
    \edge {R} {R2};
    \edge {G} {G2};
    \edge {B} {B2};
    \path[->] (R) edge[bend left] node [right] {} (R3);
    \path[->] (G) edge[bend left] node [right] {} (G3);
    \path[->] (B) edge[bend left] node [right] {} (B3);
  }}
  \caption{Hybrid Bayesian Network Model Architecture, for Pixel $ij$ with color channels $RGB$}
  \label{fig:net-structure}
\end{figure}

\subsection{Conditional Random Fields (CRFs)}
\label{sec:CRF}
Conditional Random Fields (CRFs) are discriminative graphical models that
can model output variables $\mathbf{y}$ conditioned on features $\mathbf{x}$
compactly, in contrast to generative models. CRFs are natural extensions
to Logistic Regression for general graphs. Our formulation,
parameter estimation, and inference approach comes from McCallum et al. \cite{McCallumCRF}
and Lafferty et al. \cite{CRF}.

CRFs model a conditional likelihood $p(\mathbf{y} \mid \mathbf{x})$ on a general
graph $\mathcal{G}$, composed of a set of factors
$F = \{ \psi_a \} _{a=1}^{A}$. We reiterate McCallum's description
of a CRF with parameter tying, where a set of factors can share a set of feature
functions $\{f_{pk}\left(\mathbf{x}_c, \mathbf{y}_c \right)\}_{k=1}^{K(p)}$ and
parameters $\theta_p \in \mathbb{R}^{K(p)}$ for a given \textit{clique template}
$C_p$. These clique templates come from partitioning the factors of graph
$\mathcal{G}$ into $\mathcal{C} = \{C_1, C_2, \hdots, C_p \}$. Hence, the model
is composed of the following:
\begin{equation}
  \begin{aligned}
    p(\mathbf{y} \mid \mathbf{x}) \; = \; & \frac{1}{Z(\mathbf{x})} \prod_{C_p \in \mathcal{C}} \prod_{\mathbf{\psi}_c \in C_p} \psi_c (\mathbf{x}_c, \mathbf{y}_c ; \theta_p) \\
    \psi_c (\mathbf{x}_c, \mathbf{y}_c ; \theta_p ) \; = \; & \exp \Bigg\{ \sum_{k=1}^{K(p)} \theta_{pk} f_{pk} (\mathbf{x}_c, \mathbf{y}_c) \Bigg\} \\
    Z(\mathbf{x}) \; = \; & \sum_{\mathbf{y}} \prod_{C_p \in \mathcal{C}} \prod_{\mathbf{\psi}_c \in C_p} \psi_c (\mathbf{x}_c, \mathbf{y}_c ; \theta_p) \\
  \end{aligned}
\end{equation}
Here, $p$ is a factor index, $f_{pk}$ is a feature function, and $\theta_{pk}$ is the
weight associated with it. $K$ is the number of features and $K(p)$ are the
features found in factor $p$.
Our labels are discrete and can be written as
feature functions $f_{pk}$ composed  of observation functions $q_{pk}$ to make
\textit{label-observation features}.
\begin{equation}
  f_{pk} (\mathbf{y}_c, \mathbf{x}_c) = \mathds{1} \{\mathbf{y}_c = \mathbf{\widetilde{y}}_c\} \cdot q_{pk}(\mathbf{x}_c)
\end{equation}

Parameter estimation for CRFs is done through MLE,
where we can minimize the conditional negative log likelihood:
\begin{equation}
  \ell (\theta) \; = \; - \sum_{C_p \in \mathcal{C}} \sum_{\psi_c \in C_p} \sum_{k=1}^{K(p)} \theta_{pk} f_{pk} (\mathbf{x}_c, \mathbf{y}_c) + \log Z(\mathbf{x})
\end{equation}
The partial derivative with respect to the parameter $\theta_{pk}$ associated
with the clique template $C_p$ is:
\begin{equation}
  \frac{\partial \ell}{\partial \theta_{pk}} \; = \; - \sum_{\psi_c \in C_p} f_{pk} (\mathbf{x}_c, \mathbf{y}_c) + \sum_{\psi_c \in C_p} \sum_{\mathbf{y'}_c} f_{pk} (\mathbf{x}_c, \mathbf{y'}_c) \cdot p(\mathbf{y'}_c \mid \mathbf{x})
\end{equation}
Minimization of the cost function $\ell(\theta)$ is done through the L-BFGS
optimization algorithm \cite{Nowozin}.

% Add inference

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Experimental Task}
\label{sec:task}
To effectively study these objective measures, we conduct this experiment
per model per downsample size. Our choices of sizes
included: $1$, $2$, $4$, $8$, $16$, $32$, $64$, and $128$. Larger image sizes,
such as $256 \times 256$, quickly became impossible due to memory constraints.
For the HBN, any test larger than $16 \times 16$ was intractable
due to high prediction costs.

For a single downsample size, we performed a $3$-fold cross validation on
the full $9,076$ image set, using $908$
images randomly as test. This gives three point estimates from which
the standard deviation can be derived across the folds. Variables measured for the
cross-validation include: training accuracy, testing accuracy, fit time, and
score time. These measures provide two fronts of analysis: generalization and
tractability.

All images were rescaled such that its pixel color channels ranged from
$[0, 1]$, and we assume the data is continuous to avoid data
fragmentation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results: Baseline Models}

\label{sec:results-baseline}

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3cm]{nb_graph_score.png}
    \caption*{}
    \label{fig:nb_graph_score}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3cm]{nb_graph_time.png}
    \caption*{}
    \label{fig:nb_graph_time}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3cm]{nb_graph_time_score.png}
    \caption*{}
    \label{fig:nb_graph_time_score}
  \end{subfigure}
  \vspace{-20pt}
  \caption{Generalization and Tractability Results for Gaussian Naive Bayes}
  \label{fig:GNB}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3cm]{lr_graph_score.png}
    \caption*{}
    \label{fig:lr_graph_score}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3cm]{lr_graph_time.png}
    \caption*{}
    \label{fig:lr_graph_time}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3cm]{lr_graph_time_score.png}
    \caption*{}
    \label{fig:lr_graph_time_score}
  \end{subfigure}
  \vspace{-20pt}
  \caption{Generalization and Tractability Results for Multinomial Logistic Regression}
  \label{fig:MLR}
\end{figure}
The baseline models, while naive in its assumptions, did reasonably well in
terms of generalization ability and tractability.
The Gaussian Naive Bayes model (Figure \ref{fig:GNB}),
performed similarly in the training and testing phase.
However, compared to Logistic Regression
(Figure \ref{fig:MLR}), the Naive Bayes model takes longer to converge to a
stationary accuracy as the number of features grows.

The time to fit the Naive Bayes was significantly smaller than the multinomial
Logistic Regression. This effect stems from the lack of a quasi second-order
optimization approach that Logistic Regression uses through L-BFGS. However,
Logistic Regression was faster in inference, since Naive Bayes computes
the gaussian likelihood for every feature $x_{ijk}$ to make a prediction.

From Figures \ref{fig:GNB} and \ref{fig:MLR}, no noticeable gains
are made for either model for downsampled images larger than $16 \times 16$,
and in the case of Logistic Regression, can even overfit to noise.
The best explanation is that image interpolation creates robust, strong features
from raw data, reducing noise and helping generalization. However,
it is important to note the significant loss in accuracy when the image is
decimated to smaller sizes than $16 \times 16$ as information is lost. While
the models do surprising well with a single color, these results show that
image interpolation can effectively improve generalization and tractability of
raw image data given a reasonable interpolation size.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results: General Models}
\label{sec:results-general}
For the Hybrid Bayesian Network (Figure \ref{fig:HBN}), initial generalization
ability was improved from inter-feature connections.
However, this formulation proved to be intractable
after $32 \times 32$. Hence, while accuracy scored improved through
complex interactions (Figure \ref{fig:net-structure}), this is
offset by increased inference times. Parameter estimation proved to be effective,
as each node learned a small subset of coefficients through linear regression
on continuous parents and induced sparsity from our architecture.
However, inference time is significantly larger than either the
Conditional Random Field or the baselines.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3cm]{bn_graph_score.png}
    \caption*{}
    \label{fig:bn_graph_score}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3cm]{bn_graph_time.png}
    \caption*{}
    \label{fig:bn_graph_time}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3cm]{bn_graph_time_score.png}
    \caption*{}
    \label{fig:bn_graph_time_score}
  \end{subfigure}
  \vspace{-20pt}
  \caption{Generalization and Tractability Results for a Hybrid Bayesian Network}
  \label{fig:HBN}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3cm]{crf_graph_score.png}
    \caption*{}
    \label{fig:crf_graph_score}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3cm]{crf_graph_time.png}
    \caption*{}
    \label{fig:crf_graph_time}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3cm]{crf_graph_time_score.png}
    \caption*{}
    \label{fig:crf_graph_time_score}
  \end{subfigure}
  \vspace{-20pt}
  \caption{Generalization and Tractability Results for a Conditional Random Field}
  \label{fig:CRF}
\end{figure}

The CRF (Figure \ref{fig:CRF}) took longer to train than
any other model, and yet
had a comparable generalization curve to Logistic Regression, while outperforming
the generative models. Logistic Regression performed better in training, yet
had similar test scores. The increased train times result from
a more complicated optimization problem, using L-BFGS. In contrast,
inference was faster than the Hybrid Bayesian Network.

\section{Results: General Findings}
Discriminative models learned better than their generative counterparts, as they
model the conditional probability $p(y\mid \mathbf{x})$ directly. Hence, they are
less concerned with modeling the features, and results in a lower parameter count.
However, they are susceptible to over-fitting.

Image interpolation can effectively reduce the size of the data and models
by creating robust features invariant to noise. Caution must be exerted if too
much information is loss. Results
stagnated or degraded as the feature count increased.
% This is known as
% the \textit{Hughes Phenomenon}, which states that as the number
% of features increases, our performance will initially increase until optimality
% before degrading due to the sparsity of the observed feature space \cite{hp1} \cite{hp2}.

Adding more complex interactions did not help the learning task, and in some cases
hindered our performance in generalization and tractability. This suggests that image
interpolation encoded all relevant interactions between pixels in the original
image that do not need to be accounted for with general graphs.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
\label{sec:conclusion}
Our results show that image interpolation can be a useful technique
for dimensionality reduction when using graphical models for classification.
Our original images of $500 \times 500$ contained approximately $750,000$ features,
and interpolation can yield comparable results when reduced to a size of
$16 \times 16$ ($768$ features). Image decimation is still prone to information
loss and provides sub-optimal performance in heavily downsized images. However,
our results indicate that interpolation can be an attractive dimensionality
reduction technique that preserves structure while improving generalization and
tractability of graphical models.

All code for this project can be found here: \url{https://github.com/nextBillyonair/PGMProject}

% talk about discrim vs gen

\section{Future Work}
An extension of these experiments would be to examine the effect of color
quantization, a technique used to reduce the number of colors in an image. This
would compress the feature space for an image. Another extension would
be to use structure learning algorithms on decimated images to explore
what interactions are supported by the underlying distributions. Finally,
examining how inter-class confusion in inference is influenced by interpolation
techniques could yield interesting insights in what the models learn.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% ------------------------------



\bibliographystyle{abbrv}
\bibliography{report}

% \appendix
% \section{Number of Parameters}
% \subsection{Baseline Models}
% Since our baseline models assume independence between each feature, the formulas
% for calculating the number of parameters is easy.
% \begin{equation}
%   \begin{aligned}
%     \text{Naive Bayes}(s, y) \; = & \; 2 \cdot y \cdot (3 s^2) + y \\
%     \text{Logistic Regression}(s, y) \; = & \; 3 s^2 + y \\
%   \end{aligned}
% \end{equation}
% Here, $y$ is the number of classes, and $s$ is the downsample size, i.e. 1, 2, 16.
% \subsection{General Models}
% The general case is more difficult, as parameters are now dependent on the number
% of parents. For hybrid models of discrete and continuous data, assuming one
% discrete node with $y$ classes and continuous nodes $x_i \in \mathbf{x}$. Let
% $|pa_c(x_i)|$ represent the number of continuous parents of $x_i$. The number of
% parameters for a hybrid model of $\mathbf{x}$ nodes is:
% \begin{equation}
%   \text{Hybrid Model}(\mathbf{x}, y) \; = \; y + 2 \cdot y \cdot \sum_{x_i \in \mathbf{x}} \big( 1 + |pa_c (x_i)| \big)
% \end{equation}

\end{document}
