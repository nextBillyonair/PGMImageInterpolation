\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usepackage{color}
\usepackage{array}
\usepackage{dsfont}
\usepackage{multirow, graphicx}
 \usepackage{float}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\usepackage{caption}
\usepackage{subcaption}
\urlstyle{same}

\title{The Effect of Image Interpolation on the Generalization of Generative and Discriminative Models}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  William Watson \\
  The Johns Hopkins University\\
  \texttt{billwatson@jhu.edu} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  In statistical classification, there are two main approaches to learning:
  \textit{generative} and \textit{discriminative}. In computer vision,
  \textit{image interpolation} is a method to rescale images analogous to
  dimensionality reduction. We explore the generalization and tractability of
  \textit{generative-discriminative pairs} when interpolating images to smaller
  sizes.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Minimum length: 4 pages. Maximum length: 8 pages !!!!!                   %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}
\label{sec:intro}
% something about how this might be interesting
Probabilistic graphical models have many applications in computer science,
ranging from image segmentation to named entity recognition. However, these
models can become unwieldy when our feature space $\mathbf{x}$ grows. In the
case of images, as we increase the number of pixels per row and column, the
total number of features grows exponentially. For large images, if we treat
the input space as a rasterized image, then the total number of features is
$3 \times size^{2}$.

This leads to issues of tractability for both parameter estimation and
inference. This paper concerns itself with using image interpolation as
a dimensionality reduction technique to make learning tractable. We vary the
dimensionality to see both the generalization ability of our models and the
efficiency of parameter estimation. We describe our data in Section \ref{sec:data},
our experimental procedure in Section \ref{sec:task}, and image interpolation
(Section \ref{sec:img-interpolation}).

We experiment with this concept through two approaches: \textit{generative}
and \textit{discriminative} learning. Both approaches are outlined in Section
\ref{sec:classification}. Our models are formally described in Section
\ref{sec:baseline-models} and Section \ref{sec:general-models}. Our results
are presented for all models experiments (Section \ref{sec:results-baseline} and
\ref{sec:results-general}).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% steal from proposal
\section{Statistical Classification}
\label{sec:classification}
Classification is the task of assigning a label $y$ to a set of observed
features $\mathbf{x}$. Yet the way we approach modeling these relationships
can be broken down into either the \textit{generative} or the
\textit{discriminative} approach.

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \scalebox{0.8}{
        \tikz{ %
          \node[latent] (y) {$y$} ; %
          \node[obs, below=of y] (x) {$\mathbf{x}$} ; %
          \edge {y} {x} ; %
        }}
        \caption*{Generative}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \scalebox{0.8}{
        \tikz{ %
          \node[obs] (x) {$\mathbf{x}$} ; %
          \node[latent, below=of x] (y) {$y$} ; %
          \node[factor, below=of x] (factor-node) {} ; %
          \factoredge[-] {x} {factor-node} {y} ; %
        }}
        \caption*{Discriminative}
    \end{subfigure}
\end{figure*}

The generative approach models the joint distribution $p(y, \mathbf{x})$,
and can assign labels through Bayes rule \cite{NgJordan}.
\begin{equation}
    \hat{y} \; = \; \argmax_y \; p(y, \mathbf{x}) \; = \; \argmax_y \; p(y) \cdot p(\mathbf{x} | y) \\
\end{equation}
Generative models are attractive in that distributions are learned over the
feature space for each individual class. However, an important limitation
mentioned by McCallum et. al \cite{McCallumCRF} is that modeling a distribution
\textit{per feature} quickly becomes intractable as the dimensionality of our
observed variables $\mathbf{x}$ grows. While simple models can mitigate
these issues by assuming independence among the features, allowing complex
dependencies between inputs offers the ability to increase performance.

An alternative approach is to model the conditional probabilities directly,
ignoring the feature distributions. This is the discriminative approach,
and is sometimes referred to as a \textit{distribution-free classifier}.
By ignoring the feature distributions, parameters are learned only on the
conditional likelihood $p(y | \mathbf{x})$, resulting in a compact model
that can handle large feature spaces, as dependencies
are ignored \cite{McCallumCRF}.

To compare generative and discriminative modeling, one can experiment
with pairs of classifiers that can be considered analogous to each other.
More formally, a \textit{generative-discriminative pair} is a parametric
family of probabilistic models that can either be fit to maximize the joint
probability $p(y, \mathbf{x})$ or the conditional
likelihood $p(y | \mathbf{x})$ \cite{NgJordan}. The simplest pairing is
the Naive Bayes classifier (generative) and Logistic Regression
(discriminative). This paradigm can be extended to sequential and general
models to form more pairs. We discuss our experimental pairs
in Sections \ref{sec:baseline-models} and \ref{sec:general-models}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Image Interpolation}
\label{sec:img-interpolation}
% Explain process and how its good or cool, analogy to dimensionality reduction
% in a different way
The most common problem encountered in machine learning is the
\textit{curse of dimensionality}. Essentially, as our feature space
$\mathbf{x}$ grows in size, we require a larger number of parameters to
estimate. This can quickly become intractable in complex models, and
reasonably difficult in simple ones. We can see the growth
in features as we increase the image size in Figure \ref{fig:feature-size}.
One method to reduce the number of
features for a data set is Principle Component Analysis, which projects
data to a smaller dimension while maximizing the variance. However,
interpretability is lost, and the structure of the painting is unintelligible.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[height=3.5cm]{feature_graph.png}
    \caption{Growth in the Number of Features}
    \label{fig:feature-graph}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[height=3.5cm]{param_graph.png}
    \caption{Growth in the Number of Model Parameters}
    \label{fig:param-graph}
  \end{subfigure}
  \caption{As the image size increases, the problem quickly becomes intractable}
  \label{fig:feature-size}
\end{figure}

Image interpolation offers a convenient way to downsample images according to
their local neighborhood, aggregating otherwise noisy estimates into a single
value. This may be beneficial for classification tasks, as we combine a method
analogous to dimensionality reduction while preserving the fundamental structure of
the original content. We use OpenCV's implementation of interpolation \cite{opencv}.

Formally, image interpolation is the task of rescaling an image of one size
to another. This can be used to both decimate and expand the image. Several
methods exists for this task, including linear, bicubic, nearest neighbor, and
\textit{area relation}.

% downsize this
\begin{figure}[h!]
  \centering
  \caption{Results of Interpolating a 249,000 pixel image to a 4,096 pixel image.}
  \label{fig:interpolation}
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[height=4cm]{ori_img.png}
    \vspace{-13pt}
    \caption{Original Image ($498 \times 500$)}
    \label{fig:ori-image}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[height=4cm]{decimated_img.png}
    \vspace{-13pt}
    \caption{Decimated Image ($64 \times 64$)}
    \label{fig:reduced-img}
  \end{subfigure}

  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[height=2.45cm]{ori_pixel_plot.png}
    \caption{Pixel Plot for the Original Image}
    \label{fig:ori-pixel-plot}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[height=2.45cm]{decimated_pixel_plot.png}
    \caption{Pixel Plot for the Decimated Image}
    \label{fig:reduced-pixel-plot}
  \end{subfigure}
\end{figure}

The preferred method for image decimation, or downsampling, is to
resampling using pixel area relation as it gives moire'-free results. We can
see in Figure \ref{fig:interpolation} that the general space of the pixels in
relation to their values is replicated, albeit with information loss.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Data}
\label{sec:data}
Our data is drawn from a subset of the Rijkmuseum Dataset \cite{Rijksmuseum}.
In order to have enough samples for a challenging classification task, we
elected to perform type categorization. We used the top $4$ art types: paintings,
photos, drawings, and prints. Each category is limited to the size of the
smallest set, which is $2,269$ photos. Hence we have a dataset of $9,076$ images
split evenly between the 4 classes. For training purposes, we use $10$\% of the
data as our test set ($8,168$ training, $908$ test).

 \begin{figure}[h!]
   \caption{Class Examples}
   \vspace{-5pt}
   \label{fig:examples}
   \begin{subfigure}[b]{0.23\textwidth}
     \centering
     \includegraphics[height=2.5cm]{sample_painting.png}
     \caption{Sample Painting}
     \label{fig:painting}
   \end{subfigure}
   ~
   \begin{subfigure}[b]{0.23\textwidth}
     \centering
     \includegraphics[height=2.5cm]{sample_photo.png}
     \caption{Sample Photo}
     \label{fig:photo}
   \end{subfigure}
   ~
   \begin{subfigure}[b]{0.23\textwidth}
     \centering
     \includegraphics[height=2.5cm]{sample_drawing.png}
     \caption{Sample Drawing}
     \label{fig:drawing}
   \end{subfigure}
   ~
   \begin{subfigure}[b]{0.23\textwidth}
     \centering
     \includegraphics[height=2.5cm]{sample_print.png}
     \caption{Sample Print}
     \label{fig:print}
   \end{subfigure}
 \end{figure}

From Figure \ref{fig:examples}, we can see that paintings are vastly different
from the other classes, while photos and prints may be difficult to distinguish
between. Photos and prints share a similar hue and tone, and drawings do not
always have the vibrant colors found in paintings. Hence, our 4-way
classification task should be difficult to learn, allow ample room to experiment
with complex models.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Describe our pairs
\section{Baseline Models}
\label{sec:baseline-models}
\subsection{Gaussian Naive Bayes}
\label{sec:NB}
Naive Bayes is a simple generative classifier based on applying Bayes' theorem
to extract the conditional likelihood $p(y \mid \mathbf{x})$. Its' fundamental
assumption for the features is "naive" in that it assumes that every pair of
features is conditionally independent given then class label $y$ for $K$ features.

We can formulate our model from the joint distribution:
\begin{equation}
    p(y, \mathbf{x}) \; = \; p(y) \cdot \prod_{k=1}^K p(x_k \mid y)
\end{equation}
Our task is to find the class label $y$ that maximizes the posterior conditional
likelihood:
\begin{equation}
  \begin{aligned}
    \hat{y} \; = \; & \argmax_y \; p(y | \mathbf{x}) \\
    = \; & \argmax_y \; \frac{p(y) \cdot \prod_{k=1}^K p(x_k \mid y)}{p(\mathbf{x})} \\
    = \; & \argmax_y \; p(y) \cdot \prod_{k=1}^K p(x_k \mid y)
  \end{aligned}
\end{equation}
We make use of the conditional independence assumption that all features
are solely dependent on $y$ alone, and that $p(\mathbf{x})$ is constant.

Here, $p(y)$ is the frequency of class $y$ is the training set.
For our purposes we assume the features are normally distributed per class.
This is beneficial for learning, as we learn class-wise means and variances,
in contrast to the discrete case where each variable can take on $256$ states.
Hence, our conditional feature likelihood is:
\begin{equation}
  p(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma^2_y}\right)
\end{equation}

To learn a Gaussian Naive Bayes model, we use Maximum Likelihood Estimates for
the parameters $\mu_{kc}$ and $\sigma_{kc}$ per feature $k$ and class $c$, and class weights
$\theta_c$. Hence the MLE estimates for $N$ samples are:
\begin{equation}
  \begin{aligned}
    \hat{\theta}_c \; = \; & \frac{1}{N} \sum_{i=1}^N \mathds{1} \left \{ y_i = y_c \right \} = \frac{N_c}{N} \\
    \hat{\mu}_{kc} \; = \; & \frac{1}{N_c} \sum_{i=1}^N \mathds{1} \left \{ y_i = y_c \right \} \cdot x_{ik} \\
    \hat{\sigma}_{kc} \; = \; & \frac{1}{N_c} \sum_{i=1}^N \mathds{1} \left \{ y_i = y_c \right \} \cdot \left( x_{ik} - \mu_{kc} \right)^2 \\
  \end{aligned}
\end{equation}
where $N_c = \sum_{i=1}^N \mathds{1} \left \{ y_i = y_c \right \} $, or the number
of data points belonging to class $c$ \cite{murphy}.
\subsection{Multinomial Logistic Regression}
\label{sec:LR}
Logistic Regression is the discriminative analog to the Naive Bayes
classifier, sometimes referred to as the \textit{maxmimum-entropy classifer}
or a normalized \textit{log-linear model}. As a discriminative model,
we can optimize directly for the conditional likelihood $p(y \mid \mathbf{x})$,
with $Z(\mathbf{x})$ as the normalizing partition function.
\begin{equation}
  p(y \mid \mathbf{x}) = \frac{1}{Z(\mathbf{x})} \exp \Bigg\{ \sum_{j=1}^K \theta_k f_k\left( y, \mathbf{x} \right) \Bigg\}
\end{equation}

McCallum et. al formulates the Logistic Regression model, parameterized by
weights $\theta \in \mathbb{R}^K$, in which a single set of weights is shared
across all the classes \cite{McCallumCRF}. This is achieved through a set of
feature functions that are nonzero for a single class and is defined as follows:
\begin{equation}
  \begin{aligned}
    f_{y', j} \left( y, \mathbf{x} \right) \; = \; & \mathds{1} \{ y' = y \} \cdot x_j \\
    f_{y'} \left(y, \mathbf{x}\right) \; = \; & \mathds{1} \{ y' = y \} \\
  \end{aligned}
\end{equation}
This notation, while not standard, mirrors our later formulation of Conditional
Random Fields in Section \ref{sec:CRF}.

For parameter estimation, we opted to use scikit-learn's implementation
of the L-BFGS algorithm, which approximates the Broyden-Fletcher-Goldfarb-Shanno
algorithm \cite{scikit-learn}. This is a quasi-Newton optimization method that approximates the
Hessian matrix to reduce memory usage. Scikit-learn optimizes the L2 penalized
multinomial loss \cite{scikit-learn} \cite{murphy}.
\begin{equation}
  \min_{\theta} \; \frac{1}{2} \cdot \theta^T \theta - \sum_{i=1}^N \Bigg[ \sum_{c=1}^C \Bigg[ \mathds{1} \{ y_i = y_c \} \cdot \sum_{k=1}^K \theta_k f_k\left( y_i, \mathbf{x}_i \right) \Bigg] - \log Z(\mathbf{x}_i)  \Bigg]
\end{equation}
where $y_i$ is the true label and $y_c$ could be any label.

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Generative: Naive Bayes}
        \scalebox{0.75}{
        \tikz{ %
          \centering
          \node[latent] (y) {$y$} ; %
          \node[obs, below=of y, xshift=-1cm] (x1) {$x_1$} ; %
          \node[obs, below=of y] (x2) {$x_2$} ; %
          \node[obs, below=of y, xshift=1cm] (x3) {$x_3$} ; %
          \edge {y} {x1, x2, x3} ; %
        }}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Generative: General Bayesian Networks}
        \scalebox{0.75}{
        \tikz{ %
          \centering
          \node[latent] (x1) {$x_1$} ; %
          \node[latent, right=of x1] (x2) {$x_2$};
          \node[latent, right=of x2] (x3) {$x_3$};
          \node[obs, below=of x1] (x4) {$x_4$} ; %
          \node[obs, right=of x4] (x5) {$x_5$} ; %
          \node[obs, right=of x5] (x6) {$x_6$} ; %

          \edge {x1} {x4};
          \edge {x4} {x5};
          \edge {x2} {x5,x6,x3};
          \edge {x3} {x6};
          \path[->] (x1) edge[bend left] node [right] {} (x3);
        }}
    \end{subfigure}

    \vspace{15pt}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Discriminative: Logistic Regression}
        \scalebox{0.75}{
        \tikz{ %
          \centering
          \node[latent] (y) {$y$} ; %
          \node[obs, below=of y, xshift=-1cm] (x1) {$x_1$} ; %
          \node[obs, below=of y] (x2) {$x_2$} ; %
          \node[obs, below=of y, xshift=1cm] (x3) {$x_3$} ; %
          \node[factor, below=of y, xshift=-0.5cm] (factor-node-1) {} ; %
          \node[factor, below=of y] (factor-node-2) {} ; %
          \node[factor, below=of y, xshift=0.5cm] (factor-node-3) {} ; %
          \factoredge[-] {y} {factor-node-1} {x1}; %
          \factoredge[-] {y} {factor-node-2} {x2}; %
          \factoredge[-] {y} {factor-node-3} {x3}; %
        }}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Discriminative: Conditional Random Fields}
        \scalebox{0.75}{
        \tikz{ %
          \centering
          \node[latent] (x1) {$x_1$} ; %
          \node[latent, right=of x1, yshift=-0.7cm] (x2) {$x_2$};
          \node[latent, right=of x2, yshift=0.7cm] (x3) {$x_3$};
          \node[obs, below=of x1] (x4) {$x_4$} ; %
          \node[obs, right=of x4, xshift=0.5cm] (x5) {$x_5$} ; %
          \node[obs, below=of x3] (x6) {$x_6$} ; %
          \node[factor, below=of x1] (f1) {};
          \node[factor, above=of x2, yshift=-0.15cm] (f2) {};
          \node[factor, right=of x4, xshift=.5cm] (f3) {};
          \node[factor, below=of x3, yshift=0.15cm] (f4) {};

          \factoredge[-] {x1} {f1} {x4}; %
          \factoredge[-] {x1} {f2} {x3, x2}; %
          \factoredge[-] {x4} {f3} {x2, x5}; %
          \factoredge[-] {x3} {f4} {x2, x6}; %
        }}
    \end{subfigure}
\end{figure}

\section{General Models}
\label{sec:general-models}
\subsection{Hybrid Bayesian Networks}
\label{sec:BN}
General Bayesian Networks (GBNs) are directed graphical models that are
comprised of a set of nodes and edges. The nodes are our random variables,
and they interrelate through directed edges. This allows us to
encode complex relations between our features $\mathbf{x}$ that are not
captured by the Naive Bayes classifier. We can exploit these relationships
to decompose the joint distribution into factors conditioned on the node's
parents \cite{murphy}.
\begin{equation}
  p(\mathbf{x}) \; = \; \prod_{k=1}^K p(x_k \mid pa(x_k))
\end{equation}

Since our labels are discrete and the data is continuous, we use
Hybrid Bayesian Networks. As defined in Koller \& Friedman \cite{koller},
these networks are known as \textit{Conditional Linear Gaussian (CLG)} models if
every discrete variable has only discrete parents and every continuous variable
is represented as a CLG Conditional Probability Distribution (CPD).

Continuous nodes make use of the multivariate gaussian distribution, parameterized
by a mean vector $\mathbf{\mu}$ and a symmetric square covariance matrix
$\mathbf{\Sigma}$.
\begin{equation}
  p(\mathbf{x}; \mathbf{\mu}, \mathbf{\Sigma}) \; = \; \frac{1}{(2 \pi)^{n/2} \mid \Sigma \mid^{1/2}} \exp \Big\{ -\frac{1}{2} (\mathbf{x} - \mathbf{\mu})^T \Sigma^{-1} (\mathbf{x} - \mathbf{\mu})  \Big\}
\end{equation}
The conditional linear Gaussian CPD for a continuous variable $x_j$, discrete
parents $\mathbf{U} = \{U_1, \hdots, U_M\}$, continuous parents
$\mathbf{C} = \{C_1, \hdots, C_K\}$ is, for every value
$\mathbf{u} \in Val(\mathbf{U})$ with $k+1$ coefficients
$a_{\mathbf{u},0}, \hdots, a_{\mathbf{u}, k}$ and variance
$\sigma^2_{\mathbf{u}}$:
\begin{equation}
  p(x_j \mid \mathbf{u}, \mathbf{c}) \; = \; \mathcal{N} \Bigg(a_{\mathbf{u},0} + \sum_{k=1}^K a_{\mathbf{u},k} \cdot c_k \; ; \; \sigma^2_{\mathbf{u}}  \Bigg)
\end{equation}
The log likelihood function $\ell$ for a continuous node $X_i$ is:
\begin{equation}
  \ell \; = \; \log \mathcal{L} \; = \; \sum_{i=1}^{N} \Bigg\{ -\frac{1}{2} \log (2\pi \sigma^2_{\mathbf{u}}) - \frac{1}{2} \cdot \frac{1}{\sigma^2_{\mathbf{u}}} \Bigg[ a_{\mathbf{u},0} + \sum_{k=1}^K a_{\mathbf{u},k} \cdot c_k^i - x_j^i \Bigg]^2 \Bigg\}
\end{equation}
The MLE parameters are obtained by ordinary linear regression of each node $x_j$
on its continuous parents $c \in \mathbf{C}$, for each class value
$\mathbf{u} \in \{0, 1, 2, 3\}$ \cite{koller}.

In contrast, our discrete node is the class label $y$, which is just the frequency.
The maximum likelihood estimate is:
\begin{equation}
  \hat{\theta}_{y_c} = \frac{ \sum_{i=1}^{N} \mathds{1} \{ y^{i} = y_c \} }{ \sum_{i=1}^{N} \sum_{y_k} \mathds{1} \{y^i = y_k\} } = \frac{\sum_{i=1}^{N} \mathds{1} \{ y^{i} = y_c \}}{N}
\end{equation}
Here, $y_c$ is the class being estimated, $y^i$ is the label for the $i$-th sample,
and $y_k$ are the other classes. $N$ is the number of samples.

We elected to use a hybrid model as the discrete case led to
\textit{data fragmentation}, and not every state configuration can be seen
in the training set.

Inference is performed by averaging likelihood weighting simulations using
$n=500$ random samples. For discrete values, the prediction is the highest
conditional probability. The predicted value of a continuous value is the
expected value of the conditional distribution.

\begin{figure}
  \centering
  \scalebox{0.75}{
  \tikz{ %
    \centering
    \node (R) {$R_{i,j}$} ; %
    \node[right=of R] (G) {$G_{i,j}$};
    \node[right=of G] (B) {$B_{i,j}$};
    \node[below=of R] (R2) {$R_{i+1, j}$} ; %
    \node[below=of G] (G2) {$G_{i+1, j}$} ; %
    \node[below=of B] (B2) {$B_{i+1, j}$} ; %
    \node[right=of B] (R3) {$R_{i, j+1}$} ; %
    \node[right=of R3] (G3) {$G_{i, j+1}$} ; %
    \node[right=of G3] (B3) {$B_{i, j+1}$} ; %

    \edge {R} {G};
    \path[->] (R) edge[bend left] node [right] {} (B);
    \edge {G} {B};
    \edge {R} {R2};
    \edge {G} {G2};
    \edge {B} {B2};
    \path[->] (R) edge[bend left] node [right] {} (R3);
    \path[->] (G) edge[bend left] node [right] {} (G3);
    \path[->] (B) edge[bend left] node [right] {} (B3);
  }}
  \caption{Hybrid Bayesian Network Model Architecture, for Pixel $ij$ with color channels $RGB$}
  \label{fig:net-structure}
\end{figure}

\subsection{Conditional Random Fields (CRFs)}
\label{sec:CRF}
Conditional Random Fields (CRFs) are discriminative graphical models that
can model output variables $\mathbf{y}$ conditioned on features $\mathbf{x}$
compactly, in contrast to generative models. CRFs are natural extensions
to Logistic Regression for general graphs. We describe the model formulation,
parameter estimation, and inference tasks as outlined by McCallum et. al \cite{McCallumCRF},
and introduced by Lafferty et. al \cite{CRF}.

CRFs model a conditional likelihood $p(\mathbf{y} \mid \mathbf{x})$ on a general
graph $\mathcal{G}$, composed of a set of factors
$F = \{ \psi_a \} _{a=1}^{A}$. We reiterate McCallum's description
of a CRF with parameter tying, where a set of factors can share a set of feature
functions $\{f_{pk}\left(\mathbf{x}_c, \mathbf{y}_c \right)\}_{k=1}^{K(p)}$ and
parameters $\theta_p \in \mathbb{R}^{K(p)}$ for a given \textit{clique template}
$C_p$. These clique templates come from partitioning the factors of graph
$\mathcal{G}$ into $\mathcal{C} = \{C_1, C_2, \hdots, C_p \}$. Hence, our model
is composed of the following:
\begin{equation}
  \begin{aligned}
    p(\mathbf{y} \mid \mathbf{x}) \; = \; & \frac{1}{Z(\mathbf{x})} \prod_{C_p \in \mathcal{C}} \prod_{\mathbf{\psi}_c \in C_p} \psi_c (\mathbf{x}_c, \mathbf{y}_c ; \theta_p) \\
    \psi_c (\mathbf{x}_c, \mathbf{y}_c ; \theta_p ) \; = \; & \exp \Bigg\{ \sum_{k=1}^{K(p)} \theta_{pk} f_{pk} (\mathbf{x}_c, \mathbf{y}_c) \Bigg\} \\
    Z(\mathbf{x}) \; = \; & \sum_{\mathbf{y}} \prod_{C_p \in \mathcal{C}} \prod_{\mathbf{\psi}_c \in C_p} \psi_c (\mathbf{x}_c, \mathbf{y}_c ; \theta_p) \\
  \end{aligned}
\end{equation}
Where $p$ is a factor index, $f_{pk}$ is a feature function, and $\theta_{pk}$ is the
weight associated with it. $K$ is the number of features and $K(p)$ are the
features found in factor $p$.
Because our labels are discrete, we can write our
feature functions $f_{pk}$ in terms of observation functions $q_{pk}$ to make
\textit{label-observation features}.
\begin{equation}
  f_{pk} (\mathbf{y}_c, \mathbf{x}_c) = \mathds{1} \{\mathbf{y}_c = \mathbf{\widetilde{y}}_c\} \cdot q_{pk}(\mathbf{x}_c)
\end{equation}

Parameter estimation for CRFs can be done through maximum likelihood estimation,
where we can minimize the conditional negative log likelihood:
\begin{equation}
  \ell (\theta) \; = \; - \sum_{C_p \in \mathcal{C}} \sum_{\psi_c \in C_p} \sum_{k=1}^{K(p)} \theta_{pk} f_{pk} (\mathbf{x}_c, \mathbf{y}_c) + \log Z(\mathbf{x})
\end{equation}
Hence, the partial derivative with respect to our paramter $\theta_{pk}$ associated
with the clique template $C_p$ is:
\begin{equation}
  \frac{\partial \ell}{\partial \theta_{pk}} \; = \; - \sum_{\psi_c \in C_p} f_{pk} (\mathbf{x}_c, \mathbf{y}_c) + \sum_{\psi_c \in C_p} \sum_{\mathbf{y'}_c} f_{pk} (\mathbf{x}_c, \mathbf{y'}_c) \cdot p(\mathbf{y'}_c \mid \mathbf{x})
\end{equation}
We can then minimize our cost function $\ell(\theta)$ using L-BFGS optimization
algorithm as described in Section \ref{sec:LR}.

% Add inference

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Experimental Task}
\label{sec:task}
% describe how we measure what we measure with what we have
For a single downsample size, we performed a $3$-fold cross validation on
the full $9,076$ image set, using $908$
images randomly as test. This gives us three points of accuracy, in which we
can average and extract confidence intervals around. Variables measured for the
cross-validation include: training accuracy, testing accuracy, fit time, and
score time. These measures provide two fronts of analysis: generalization and
tractability.

To effectively study these objective measures, we conduct this experiment
per model per downsample size. Our choices of sizes
included: $1$, $2$, $4$, $8$, $16$, $32$, $64$, and $128$. We did not test higher pixel sizes
as $256 \times 256$ pixel images quickly became impossible due to memory constraints.
For the general models, any test on $32 \times 32$ or larger images was intractable
due to prediction costs.

All images were rescaled such that its pixel color channels are ranged from
$[0, 1]$, and we assume our data is continuous instead of discrete to avoid data
fragmentation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results: Baseline Models}

\label{sec:results-baseline}
The baseline models, while naive in its assumptions, did reasonably well in
terms of generalization ability and tractability.


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3.2cm]{nb_graph_score.png}
    \caption*{}
    \label{fig:nb_graph_score}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3.2cm]{nb_graph_time.png}
    \caption*{}
    \label{fig:nb_graph_time}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3.2cm]{nb_graph_time_score.png}
    \caption*{}
    \label{fig:nb_graph_time_score}
  \end{subfigure}
  \vspace{-10pt}
  \caption{Generalization and Tractability Results for Gaussian Naive Bayes}
  \label{fig:GNB}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3.2cm]{lr_graph_score.png}
    \caption*{}
    \label{fig:lr_graph_score}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3.2cm]{lr_graph_time.png}
    \caption*{}
    \label{fig:lr_graph_time}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3.2cm]{lr_graph_time_score.png}
    \caption*{}
    \label{fig:lr_graph_time_score}
  \end{subfigure}
  \vspace{-10pt}
  \caption{Generalization and Tractability Results for Multinomial Logistic Regression}
  \label{fig:MLR}
\end{figure}

For the Gaussian Naive Bayes model (Figure \ref{fig:GNB}), we see two distinct trends.
First, the model performs equivalently in the training and testing phase, with
similar accuracy scores. However, compared to the Logistic Regression
(Figure \ref{fig:MLR}), the Naive Bayes model takes longer to converge to a
stationary accuracy, even when the number of features is growing exponentially.

The time to fit the Naive Bayes was also significantly smaller than the multinomial
Logistic Regression. This effect stems from the lack of a quasi second-order
optimization approach that Logistic Regression uses through L-BFGS. However,
Logistic Regression was faster in inference, since Naive Bayes computes
the gaussian likelihood for every feature $x_{ijk}$ before multiplying all
the conditional probabilities to make a prediction.

From Figures \ref{fig:GNB} and \ref{fig:MLR}, we see that no noticeable gains
are made for either model for downsampled images larger than $16 \times 16$,
and in the case of Logistic Regression, can even overfit to noise. This stems
from the fact that image interpolation creates robust, strong estimators of
individual features, reducing noise and helping generalization. However,
it is important to note the significant loss in accuracy when the image is
decimated to smaller sizes than $16 \times 16$, as information is lost. While
the models do surprising well with a single color, these results show that
image interpolation can effectively improve generalization and tractability of
raw feature training on images.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results: General Models}
\label{sec:results-general}
The general models followed similar trends to our baselines.

For the Hybrid Bayesian Network (Figure \ref{fig:HBN}), initial generalization
ability was improved
from inter-feature connections. However, this formulation proved to be intractable
after $32 \times 32$. Hence, while we improve both train and test scores through
complex interactions, detailed in Figure \ref{fig:net-structure}, this is
offset by increased inference times. Parameter estimation proved to be effective,
as each node learned a small subset of coefficients through linear regression
on continuous parents and induced sparsity. However, this formulation hindered
inference, and the time to predict is exponentially larger than either the
Conditional Random Field or the baselines.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3.2cm]{bn_graph_score.png}
    \caption*{}
    \label{fig:bn_graph_score}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3.2cm]{bn_graph_time.png}
    \caption*{}
    \label{fig:bn_graph_time}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3.2cm]{bn_graph_time_score.png}
    \caption*{}
    \label{fig:bn_graph_time_score}
  \end{subfigure}
  \vspace{-10pt}
  \caption{Generalization and Tractability Results for a Hybrid Bayesian Network}
  \label{fig:HBN}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3.2cm]{crf_graph_score.png}
    \caption*{}
    \label{fig:crf_graph_score}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3.2cm]{crf_graph_time.png}
    \caption*{}
    \label{fig:crf_graph_time}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=3.2cm]{crf_graph_time_score.png}
    \caption*{}
    \label{fig:crf_graph_time_score}
  \end{subfigure}
  \vspace{-10pt}
  \caption{Generalization and Tractability Results for a Conditional Random Field}
  \label{fig:CRF}
\end{figure}

The Conditional Random Field (Figure \ref{fig:CRF}) took longer to train than
any other model, and yet
had a comparable generalization curve to Logistic Regression, while outperforming
the generative models. Logistic Regression performed better in train score, yet
had similar test scores. The increased train and inference times result from
a more complicated model and optimization problem. Since our CRF uses L-BFGS,
the estimates to the hessian are more expensive, thus making parameter estimation
difficult. Inference was faster than the Hybrid Bayesian Network.

\section{Results: General Findings}
Discriminative models learned better than their generative counterparts, as they
model the conditional probability $p(y\mid \mathbf{x})$ directly. Hence, they are
less concerned with modeling the features, and results in a lower parameter count.

Image interpolation can effectively reduce the size of the data and models
by creating robust features invariant to noise. Caution must be exerted if too
much information is loss, and our results may be different if the original images
were of different sizes, i.e. $100 \times 100$ vs. $1000 \times 1000$.

Adding more complex interactions does not help our learning task, and in some cases
hinders our performance, both in generalization and tractability. Perhaps image
interpolation encodes all relevant interactions between pixels in the original
image that we do not need to account for with general graphs over naive assumptions.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
\label{sec:conclusion}
As we can see from our experiments, image interpolation can be a useful technique
for dimensionality reduction when using graphical models for classification.
As our original $500 \times 500$ images contained over $750,000$ features, a simple
reduction use pixel sampling yields comparable results when reduced to
$16 \times 16$ ($768$ features). However, image decimation can result in significant
generalization errors from information loss. Using image interpolation as
a dimensionality reduction technique thus preserves data intelligently and
generalization ability while improving the time it takes to perform
parameter estimation and inference in graphical models. Discriminative models
made the most gains from this technique, in contrast to generative models.

All code for this project can be found here: \url{https://github.com/nextBillyonair/PGMProject}

% talk about discrim vs gen

\section{Future Work}
Over the course if this project, several ideas would have been interesting to
explore, but where not pursued due to the scope of the research question. First,
a natural area of exploration would be to rerun the experiments using color
quantization, a technique used to reduce the number of colors in an image. This
is reduces the total number of discrete states each color channel can take on.

A second proposal would be to experiment with structure learning algorithms to
create networks that take the distribution of colors per class into account.
Our model was created with the intention of injecting sparsity while still
retaining relevant connections between pixels.

A third avenue would be to examine how interclass confusion in inference
is influenced by increasing the number of features.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% ------------------------------



\bibliographystyle{abbrv}
\bibliography{report}

% \appendix
% \section{Number of Parameters}
% \subsection{Baseline Models}
% Since our baseline models assume independence between each feature, the formulas
% for calculating the number of parameters is easy.
% \begin{equation}
%   \begin{aligned}
%     \text{Naive Bayes}(s, y) \; = & \; 2 \cdot y \cdot (3 s^2) + y \\
%     \text{Logistic Regression}(s, y) \; = & \; 3 s^2 + y \\
%   \end{aligned}
% \end{equation}
% Here, $y$ is the number of classes, and $s$ is the downsample size, i.e. 1, 2, 16.
% \subsection{General Models}
% The general case is more difficult, as parameters are now dependent on the number
% of parents. For hybrid models of discrete and continuous data, assuming one
% discrete node with $y$ classes and continuous nodes $x_i \in \mathbf{x}$. Let
% $|pa_c(x_i)|$ represent the number of continuous parents of $x_i$. The number of
% parameters for a hybrid model of $\mathbf{x}$ nodes is:
% \begin{equation}
%   \text{Hybrid Model}(\mathbf{x}, y) \; = \; y + 2 \cdot y \cdot \sum_{x_i \in \mathbf{x}} \big( 1 + |pa_c (x_i)| \big)
% \end{equation}

\end{document}
