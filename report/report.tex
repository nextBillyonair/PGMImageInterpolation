\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usepackage{color}
\usepackage{array}
\usepackage{dsfont}
\usepackage{multirow, graphicx}
 \usepackage{float}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\usepackage{caption}
\usepackage{subcaption}

\title{The Effect of Image Interpolation on the Generalization of Generative and Discriminative Models}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  William Watson \\
  Johns Hopkins University\\
  \texttt{billwatson@jhu.edu} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  In statistical classification, there are two main approaches to learning:
  \textit{generative} and \textit{discriminative}. In computer vision,
  \textit{image interpolation} is a method to rescale images analgous to
  dimensionality reduction. We explore the generalization ability of
  \textit{generative-discriminative pairs} when interpolating images to smaller
  sizes.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Minimum length: 4 pages. Maximum length: 8 pages !!!!!                   %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}
\label{sec:intro}
% something about how this might be interesting

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% steal from proposal
\section{Statistical Classification}
Classification is the task of assigning a label $y$ to a set of observed
features $\mathbf{x}$. Yet the way we approach modeling these relationships
can be broken down into either the \textit{generative} or the
\textit{discriminative} approach.

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \tikz{ %
          \node[latent] (y) {$y$} ; %
          \node[obs, below=of y] (x) {$\mathbf{x}$} ; %
          \edge {y} {x} ; %
        }
        \caption*{Generative}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \tikz{ %
          \node[obs] (x) {$\mathbf{x}$} ; %
          \node[latent, below=of x] (y) {$y$} ; %
          \node[factor, below=of x] (factor-node) {} ; %
          \factoredge[-] {x} {factor-node} {y} ; %
        }
        \caption*{Discriminative}
    \end{subfigure}
\end{figure*}

The generative approach models the joint distribution $p(y, \mathbf{x})$,
and can assign labels through Bayes rule \cite{NgJordan}.
\begin{equation}
    \hat{y} \; = \; \argmax_y \; p(y, \mathbf{x}) \; = \; \argmax_y \; p(y) \cdot p(\mathbf{x} | y) \\
\end{equation}
Generative models are attractive in that distributions are learned over the
feature space for each indvidual class. However, an important limitation
mentioned by Callum et. al \cite{McCallumCRF} is that modeling a distribution
\textit{per feature} quickly becomes intractable as the dimensionality of our
observed variables $\mathbf{x}$ grows. While simple models can mitigate
these issues by assuming independence among the features, allowing complex
dependencies between inputs offers the ability to increase performance.

An alternative approach is to model the conditonal probabilities directly,
ignoring the feature distributions. This is the discriminative approach,
and is sometimes referred to as a \textit{distribution-free classifier}.
By ignoring the feature distributions, parameters are learned only on the
conditional likelihood $p(y | \mathbf{x})$, resulting in a compact model
that can handle large feature spaces, as dependencies
are ignored \cite{McCallumCRF}.

To compare generative and discriminative modeling, one can experiment
with pairs of classifiers that can be considered analgous to each other.
More formally, a \textit{generative-discriminative pair} is a parametric
family of probabilistic models that can either be fit to maximize the joint
probability $p(y, \mathbf{x})$ or the conditional
likelihood $p(y | \mathbf{x})$ \cite{NgJordan}. The simplest pairing is
the Naive Bayes classifier (generative) and Logistic Regression
(discriminative). This paradigm can be extended to sequential and general
models to form more pairs. We discuss our experimental pairs
in Sections \ref{sec:baseline-models} and \ref{sec:general-models}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Image Interpolation}
\label{sec:img-interpolation}
% Explain process and how its good or cool, analgoy to dimensionalty reduction
% in a different way
The most common problem encountered in machine learning is the
\textit{curse of dimensionality}. Esssentially, as our feature space
$\mathbf{x}$ grows in size, we require a larger number of parameters to
estimate. This can quickly become intractable in complex models, and
reasonably difficult in simple ones. We can see the growth
in features as we increase the image size in Figure \ref{fig:feature-size}.
One method to reduce the number of
features for a data set is Principle Component Analysis, which projects
data to a smaller dimension while maximining the variance. However,
interapibility is lost, and the structure of the painting is unitnelligble.

\begin{figure}
  \centering
  \includegraphics[height=4cm]{feature_graph.png}
  \vspace{-5pt}
  \caption{Exponentional growth in the number features quickly makes modeling intractable.}
  \label{fig:feature-size}
\end{figure}

Image interpolation offers a convient way to downsample images according to
their local neighborhood, aggregating otherwise noisy estimates into a single
value. This may be beneficial for classification tasks, as we combine a method
analgous to dimsionality reduction while preserving the fundamental structure of
the orignal content. We use OpenCV's implementation of interpolation \cite{opencv}.

Formally, image interpolation is the task of rescaling an image of one size
to another. This can be used to both decimate and expand the image. Several
methods exists for this task, including linear, bicubic, nearest neighbor, and
\textit{area relation}.

\begin{figure}
  \centering
  \caption{Results of Interpolating a 249,000 pixel image to a 4,096 pixel image.}
  \label{fig:interpolation}
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[height=5cm]{ori_img.png}
    \vspace{-13pt}
    \caption{Original Image (498x500)}
    \label{fig:ori-image}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[height=5cm]{decimated_img.png}
    \vspace{-13pt}
    \caption{Decimated Image (64x64)}
    \label{fig:reduced-img}
  \end{subfigure}

  \begin{subfigure}[b]{1.\textwidth}
    \centering
    \includegraphics[height=4cm]{ori_pixel_plot.png}
    \caption{Pixel Plot for the Original Image}
    \label{fig:ori-pixel-plot}
  \end{subfigure}

  \begin{subfigure}[b]{1.\textwidth}
    \centering
    \includegraphics[height=4cm]{decimated_pixel_plot.png}
    \caption{Pixel Plot for the Decimated Image}
    \label{fig:reduced-pixel-plot}
  \end{subfigure}
\end{figure}

The perferred method for image decimation, or downsampling, is to
resampling using pixel area relation as it gives moire'-free results. We can
see in Figure \ref{fig:interpolation} that the general space of the pixels in
relation to there values is replicated, albeit with information loss.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Data}
\label{sec:data}
\subsection{Descriptive Statistics}
\label{sec:stats}
\subsection{Preprocessing Pipeline}
\label{sec:pipeline}

% talk about our data, what we reduced, some colorful stats to give
% hypothesis

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Describe our pairs
\section{Baseline Models}
\label{sec:baseline-models}
% maybe uyse paragraph
\subsection{Naive Bayes}
\label{sec:NB}
Naive Bayes is a simple generative classifier based on applying Bayes' theorem
to extract the conditional likelihood $p(y \mid \mathbf{x})$. Its' fundamental
assumption for the features is "naive" in that it assumes that every pair of
features is conditionally independent given then class label $y$ for $K$ features.

We can formulate our model from the joint distribution:
\begin{equation}
    p(y, \mathbf{x}) \; = \; p(y) \cdot \prod_{k=1}^K p(x_k \mid y)
\end{equation}
Our task is to find the class label $y$ that maximizes the posterior conditional
likelihood:
\begin{equation}
  \begin{aligned}
    \hat{y} \; = \; & \argmax_y \; p(y | \mathbf{x}) \\
    = \; & \argmax_y \; \frac{p(y) \cdot \prod_{k=1}^K p(x_k \mid y)}{p(\mathbf{x})} \\
    = \; & \argmax_y \; p(y) \cdot \prod_{k=1}^K p(x_k \mid y)
  \end{aligned}
\end{equation}
We make use of the conditional independence assumption that all features
are soely dependent on $y$ alonge, and that $p(\mathbf{x})$ is constant.

Here, $p(y)$ is the frequency of class $y$ is the training set.
For our purpuses we assume the features are normally distributed per class.
Hence, our conditonal feature likelihood is:
\begin{equation}
  p(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma^2_y}\right)
\end{equation}

To learn a Gaussian Naive Bayes model, we use Maximum Likelihood Estimates for
the parameters $\mu_{kc}$ and $\sigma_{kc}$ per feature $k$ and class $c$, and class weights
$\theta_c$. Hence the MLE estimates for $N$ samples are:
\begin{equation}
  \begin{aligned}
    \theta_c \; = \; & \frac{1}{N} \sum_{i=1}^N \mathds{1} \left \{ y_i = y_c \right \} = \frac{N_c}{N} \\
    \mu_{kc} \; = \; & \frac{1}{N_c} \sum_{i=1}^N \mathds{1} \left \{ y_i = y_c \right \} \cdot x_{ik} \\
    \sigma_{kc} \; = \; & \frac{1}{N_c} \sum_{i=1}^N \mathds{1} \left \{ y_i = y_c \right \} \cdot \left( x_{ik} - \mu_{kc} \right)^2 \\
  \end{aligned}
\end{equation}
where $N_c = \sum_{i=1}^N \mathds{1} \left \{ y_i = y_c \right \} $, or the number
of data points belonging to class $c$ \cite{murphy}.
\subsection{Logistic Regression}
\label{sec:LR}
Logistic Regression is the discriminative analog to the Naive Bayes
classifer, sometimes referred to as the \textit{maxmimum-entropy classifer}
or a normalized \textit{log-linear model}. As a discriminative model,
we can optimize directly for the conditional likelihood $p(y \mid \mathbf{x})$,
with $Z(\mathbf{x})$ as the normalizing parition function.
\begin{equation}
  p(y \mid \mathbf{x}) = \frac{1}{Z(\mathbf{x})} \exp \Bigg\{ \sum_{j=1}^K \theta_k f_k\left( y, \mathbf{x} \right) \Bigg\}
\end{equation}

McCallum et. al formulates the logisitic regression model, parameterized by
weights $\theta \in \mathbb{R}^K$, in which a single set of weights is shared
across all the classes \cite{McCallumCRF}. This is acheived through a set of
feature functions that are nonzero for a single class and is defined as follows:
\begin{equation}
  \begin{aligned}
    f_{y', j} \left( y, \mathbf{x} \right) \; = \; & \mathds{1} \{ y' = y \} \cdot x_j \\
    f_{y'} \left(y, \mathbf{x}\right) \; = \; & \mathds{1} \{ y' = y \} \\
  \end{aligned}
\end{equation}
This notation, while not standard, mirrors our later formulation of Conditional
Random Fields in Section \ref{sec:CRF}.

For parameter estimation, we opted to use scikit-learn's implementation
of the L-BFGS algorithm, which approximates the Broyden-Fletcher-Goldfarb-Shanno
algoriothm \cite{scikit-learn}. This is a quasi-Newton optimzation method that apprixmates the
Hessian matrix to reduce memory usage. Scikit-learn optimizes the L2 penalized
multinomial loss \cite{scikit-learn} \cite{murphy}.
\begin{equation}
  \min_{\theta} \; \frac{1}{2} \cdot \theta^T \theta - \sum_{i=1}^N \Bigg[ \sum_{c=1}^C \Bigg[ \mathds{1} \{ y_i = y_c \} \cdot \sum_{k=1}^K \theta_k f_k\left( y_i, \mathbf{x}_i \right) \Bigg] - \log Z(\mathbf{x}_i)  \Bigg]
\end{equation}
where $y_i$ is the true label and $y_c$ could be any label.

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Generative: Naive Bayes}
        \tikz{ %
          \centering
          \node[latent] (y) {$y$} ; %
          \node[obs, below=of y, xshift=-1cm] (x1) {$x_1$} ; %
          \node[obs, below=of y] (x2) {$x_2$} ; %
          \node[obs, below=of y, xshift=1cm] (x3) {$x_3$} ; %
          \edge {y} {x1, x2, x3} ; %
        }
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Generative: General Bayesian Networks}
        \tikz{ %
          \centering
          \node[latent] (x1) {$x_1$} ; %
          \node[latent, right=of x1] (x2) {$x_2$};
          \node[latent, right=of x2] (x3) {$x_3$};
          \node[obs, below=of x1] (x4) {$x_4$} ; %
          \node[obs, right=of x4] (x5) {$x_5$} ; %
          \node[obs, right=of x5] (x6) {$x_6$} ; %

          \edge {x1} {x4};
          \edge {x4} {x5};
          \edge {x2} {x5,x6,x3};
          \edge {x3} {x6};
          \path[->] (x1) edge[bend left] node [right] {} (x3);
        }
    \end{subfigure}

    \vspace{15pt}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Discriminative: Logistic Regression}
        \tikz{ %
          \centering
          \node[latent] (y) {$y$} ; %
          \node[obs, below=of y, xshift=-1cm] (x1) {$x_1$} ; %
          \node[obs, below=of y] (x2) {$x_2$} ; %
          \node[obs, below=of y, xshift=1cm] (x3) {$x_3$} ; %
          \node[factor, below=of y, xshift=-0.5cm] (factor-node-1) {} ; %
          \node[factor, below=of y] (factor-node-2) {} ; %
          \node[factor, below=of y, xshift=0.5cm] (factor-node-3) {} ; %
          \factoredge[-] {y} {factor-node-1} {x1}; %
          \factoredge[-] {y} {factor-node-2} {x2}; %
          \factoredge[-] {y} {factor-node-3} {x3}; %
        }
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Discriminative: Conditional Random Fields}
        \tikz{ %
          \centering
          \node[latent] (x1) {$x_1$} ; %
          \node[latent, right=of x1, yshift=-0.7cm] (x2) {$x_2$};
          \node[latent, right=of x2, yshift=0.7cm] (x3) {$x_3$};
          \node[obs, below=of x1] (x4) {$x_4$} ; %
          \node[obs, right=of x4, xshift=0.5cm] (x5) {$x_5$} ; %
          \node[obs, below=of x3] (x6) {$x_6$} ; %
          \node[factor, below=of x1] (f1) {};
          \node[factor, above=of x2, yshift=-0.15cm] (f2) {};
          \node[factor, right=of x4, xshift=.5cm] (f3) {};
          \node[factor, below=of x3, yshift=0.15cm] (f4) {};

          \factoredge[-] {x1} {f1} {x4}; %
          \factoredge[-] {x1} {f2} {x3, x2}; %
          \factoredge[-] {x4} {f3} {x2, x5}; %
          \factoredge[-] {x3} {f4} {x2, x6}; %
        }
    \end{subfigure}
\end{figure}

\section{General Models}
\label{sec:general-models}
\subsection{Bayesian Networks}
\label{sec:GBN}
\subsection{Conditonal Random Fields (CRFs)}
\label{sec:CRF}
Conditional Random Fields (CRFs) are discriminative graphical models that
can model output variables $\mathbf{y}$ conditioned on features $\mathbf{x}$
compactly, in contrast to generative models. CRFs are natural extensions
to logistic regression for general graphs. We describe the model formulation,
paramter estimation, and inference tasks as outlined by McCallum et. al \cite{McCallumCRF},
and introduced by Lafferty et. al \cite{CRF}.

CRFs model a conditional likelihood $p(\mathbf{y} \mid \mathbf{x})$ on a general
graph $\mathcal{G}$, composed of a set of factors
$F = \{ \psi_a \} _{a=1}^{A}$. We reiterate McCallum's description
of a CRF with paramter tying, where a set of factors can share a set of feature
functions $\{f_{pk}\left(\mathbf{x}_c, \mathbf{y}_c \right)\}_{k=1}^{K(p)}$ and
parameters $\theta_p \in \mathbb{R}^{K(p)}$ for a given \textit{clique template}
$C_p$. These clique templates come from paritioning the factors of graph
$\mathcal{G}$ into $\mathcal{C} = \{C_1, C_2, \hdots, C_p \}$. Hence, our model
is composed of the following:
\begin{equation}
  \begin{aligned}
    p(\mathbf{y} \mid \mathbf{x}) \; = \; & \frac{1}{Z(\mathbf{x})} \prod_{C_p \in \mathcal{C}} \prod_{\mathbf{\psi}_c \in C_p} \psi_c (\mathbf{x}_c, \mathbf{y}_c ; \theta_p) \\
    \psi_c (\mathbf{x}_c, \mathbf{y}_c ; \theta_p ) \; = \; & \exp \Bigg\{ \sum_{k=1}^{K(p)} \theta_{pk} f_{pk} (\mathbf{x}_c, \mathbf{y}_c) \Bigg\} \\
    Z(\mathbf{x}) \; = \; & \sum_{\mathbf{y}} \prod_{C_p \in \mathcal{C}} \prod_{\mathbf{\psi}_c \in C_p} \psi_c (\mathbf{x}_c, \mathbf{y}_c ; \theta_p) \\
  \end{aligned}
\end{equation}
where $p$ is a factor index, $f_{pk}$ is a feature function, and $\theta_{pk}$ is the
weight associated with it. $K$ is the number of features and $K(p)$ are the
features found in factor $p$.
Because our labels are discrete, we can write our
feature functions $f_pk$ in terms of observation functions $q_{pk}$ to make
\textit{label-observation features}.
\begin{equation}
  f_{pk} (\mathbf{y}_c, \mathbf{x}_c) = \mathds{1} \{\mathbf{y}_c = \mathbf{\widetilde{y}}_c\} \cdot q_{pk}(\mathbf{x}_c)
\end{equation}

Parameter estimation for CRFs can be done through maximum likelihood estimation,
where we can minimize the conditional negative log likelihood:
\begin{equation}
  \ell (\theta) \; = \; - \sum_{C_p \in \mathcal{C}} \sum_{\psi_c \in C_p} \sum_{k=1}^{K(p)} \theta_{pk} f_{pk} (\mathbf{x}_c, \mathbf{y}_c) + \log Z(\mathbf{x})
\end{equation}
Hence, the partial derivative with respect to our paramter $\theta_{pk}$ associated
with the clique template $C_p$ is:
\begin{equation}
  \frac{\partial \ell}{\partial \theta_{pk}} \; = \; - \sum_{\psi_c \in C_p} f_{pk} (\mathbf{x}_c, \mathbf{y}_c) + \sum_{\psi_c \in C_p} \sum_{\mathbf{y'}_c} f_{pk} (\mathbf{x}_c, \mathbf{y'}_c) \cdot p(\mathbf{y'}_c \mid \mathbf{x})
\end{equation}
We can then minimize our cost function $\ell$ using L-BFGS optimization
algorithm as described in Section \ref{sec:LR}.

% Add inference

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Experimental Task}
\label{sec:task}
% describe how we measure what we measure with what we have


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results: Parameter Estimation}
\label{sec:results-param}
\subsection{Baseline}
\subsection{General}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.47\textwidth}
    \centering
    \includegraphics[height=4.75cm]{nb_graph.png}
    \caption*{}
    \label{fig:nb_graph}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.47\textwidth}
    \centering
    \includegraphics[height=4.75cm]{lr_graph.png}
    \caption*{}
    \label{fig:lr_graph}
  \end{subfigure}
  \vspace{-10pt}
  \caption{Training/Testing Accuracy Results for Baseline Models.}
\end{figure}

\section{Results: Inference}
\label{sec:results-inference}
\subsection{Baseline}
\subsection{General}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
\label{sec:conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
% \section{Introduction}
% For a classification task, we are interested in the probability distribution
% of a class label $y$ conditioned on the sample features $\mathbf{x}$,
% written as $p(y\,|\,\mathbf{x})$. We can approach modeling this
% distribution in two ways: \textit{generative} or \textit{discriminative}.
%
% \begin{figure*}[h!]
%     \centering
%     \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \tikz{ %
%           \node[latent] (y) {$y$} ; %
%           \node[obs, below=of y] (x) {$\mathbf{x}$} ; %
%           \edge {y} {x} ; %
%         }
%         \caption*{Generative}
%     \end{subfigure}%
%     ~
%     \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \tikz{ %
%           \node[obs] (x) {$\mathbf{x}$} ; %
%           \node[latent, below=of x] (y) {$y$} ; %
%           \node[factor, below=of x] (factor-node) {} ; %
%           \factoredge[-] {x} {factor-node} {y} ; %
%         }
%         \caption*{Discriminative}
%     \end{subfigure}
% \end{figure*}
%
% \subsection{Generative Approach}
% A generative model is a statistical process to describe the observed data
% $\mathbf{x}$. More specifically, generative classifiers seek to
% model the joint probability $p(\mathbf{x},\, y)$ from our data\cite{NgJordan}.
% Predictions can be made with Bayes rule as follows:
% \begin{equation}
%   \hat{y} \,=\, \argmax_y \, p(\mathbf{x}, \,y) \,=\, \argmax_y \, p(y \,|\, \mathbf{x}) \, p(\mathbf{x}) \,=\, \argmax_y \, p(\mathbf{x} \,|\, y) \, p(y)
% \end{equation}
%
% \subsection{Discriminative Approach}
% In contrast, a discriminative model is a statistical process to describe
% relations between the observed data $\mathbf{x}$ and some latent variables
% $\mathbf{h}$. For discriminative classifiers, the latent variables are class
% labels $y$, and model the posterior distribution $p(y\,|\,\mathbf{x})$
% directly \cite{NgJordan}. Hence, prediction becomes:
% \begin{equation}
%   \hat{y} \,=\, \argmax_y \, p(y \,|\, \mathbf{x})
% \end{equation}
%
% \section{Data}
% I will use the Rijkmuseum Dataset \cite{Rijksmuseum} to perform type categorization.
% However, to make parameter estimation and inference tractable,
% the problem will be converted to a binary (paintings/photos) task. Images
% will be downsampled in size and color encoding to reduce the variable and
% state space. There are 3,583 paintings (\textit{schilderij}) and 2,269 photos
% (\textit{foto}).
%
% % to reduce word count and size this is a temp section, use what I wrote for actual papers
%
% \section{Models}
% In order to compare generative and discriminative
% approaches to classification, I will explore their static and general versions,
% as outlined in Figure \ref{fig:models}.
%
% \subsection{Baseline}
% The baseline model will consist of a \textit{Naive Bayes} classifier, a simple generative
% Bayesian network that assumes all the features $x_i \in \mathbf{x}$ are
% independent from one another and the sole parent is the class label $y$. This
% assumption simplifies the factorization for the joint distribution.
%
% The discriminative analog to Naive Bayes is \textit{Logistic Regression}, which is
% also a naive classifier, yet models the conditional probability directly.
%
% \subsection{General}
% The general counterparts to the baseline models are the generative
% \textit{General Bayesian Network} and its discriminative analog
% \textit{Conditional Random Fields (CRFs)}.
%
% General Bayesian Networks are probabilistic models that
% encode the conditional dependencies of its variables via a directed acyclic
% graph. Each variable is dependent only on its parents,
% allowing for more complex interactions between the features.
%
% CRFs are an undirected probabilistic graphical model whose
% nodes can be partitioned into two disjoint sets: $\mathbf{x}$ and $\mathbf{y}$,
% and models the conditional distribution $p(\mathbf{y}\, | \, \mathbf{x})$
% directly.
%
% \begin{figure*}[h!]
%     \centering
%     \caption{Generative vs. Discriminative Models}
%     \label{fig:models}
%     \begin{subfigure}[t]{0.45\textwidth}
%         \centering
%         \caption*{Generative: Naive Bayes}
%         \tikz{ %
%           \centering
%           \node[latent] (y) {$y$} ; %
%           \node[obs, below=of y, xshift=-1cm] (x1) {$x_1$} ; %
%           \node[obs, below=of y] (x2) {$x_2$} ; %
%           \node[obs, below=of y, xshift=1cm] (x3) {$x_3$} ; %
%           \edge {y} {x1, x2, x3} ; %
%         }
%     \end{subfigure}%
%     ~
%     \begin{subfigure}[t]{0.45\textwidth}
%         \centering
%         \caption*{Generative: General Bayesian Networks}
%         \tikz{ %
%           \centering
%           \node[latent] (x1) {$x_1$} ; %
%           \node[latent, right=of x1] (x2) {$x_2$};
%           \node[latent, right=of x2] (x3) {$x_3$};
%           \node[obs, below=of x1] (x4) {$x_4$} ; %
%           \node[obs, right=of x4] (x5) {$x_5$} ; %
%           \node[obs, right=of x5] (x6) {$x_6$} ; %
%
%           \edge {x1} {x4};
%           \edge {x4} {x5};
%           \edge {x2} {x5,x6,x3};
%           \edge {x3} {x6};
%           \path[->] (x1) edge[bend left] node [right] {} (x3);
%         }
%     \end{subfigure}
%
%     \vspace{15pt}
%     \begin{subfigure}[t]{0.45\textwidth}
%         \centering
%         \caption*{Discriminative: Logistic Regression}
%         \tikz{ %
%           \centering
%           \node[latent] (y) {$y$} ; %
%           \node[obs, below=of y, xshift=-1cm] (x1) {$x_1$} ; %
%           \node[obs, below=of y] (x2) {$x_2$} ; %
%           \node[obs, below=of y, xshift=1cm] (x3) {$x_3$} ; %
%           \node[factor, below=of y, xshift=-0.5cm] (factor-node-1) {} ; %
%           \node[factor, below=of y] (factor-node-2) {} ; %
%           \node[factor, below=of y, xshift=0.5cm] (factor-node-3) {} ; %
%           \factoredge[-] {y} {factor-node-1} {x1}; %
%           \factoredge[-] {y} {factor-node-2} {x2}; %
%           \factoredge[-] {y} {factor-node-3} {x3}; %
%         }
%     \end{subfigure}%
%     ~
%     \begin{subfigure}[t]{0.45\textwidth}
%         \centering
%         \caption*{Discriminative: Conditional Random Fields}
%         \tikz{ %
%           \centering
%           \node[latent] (x1) {$x_1$} ; %
%           \node[latent, right=of x1, yshift=-0.7cm] (x2) {$x_2$};
%           \node[latent, right=of x2, yshift=0.7cm] (x3) {$x_3$};
%           \node[obs, below=of x1] (x4) {$x_4$} ; %
%           \node[obs, right=of x4, xshift=0.5cm] (x5) {$x_5$} ; %
%           \node[obs, below=of x3] (x6) {$x_6$} ; %
%           \node[factor, below=of x1] (f1) {};
%           \node[factor, above=of x2, yshift=-0.15cm] (f2) {};
%           \node[factor, right=of x4, xshift=.5cm] (f3) {};
%           \node[factor, below=of x3, yshift=0.15cm] (f4) {};
%
%           \factoredge[-] {x1} {f1} {x4}; %
%           \factoredge[-] {x1} {f2} {x3, x2}; %
%           \factoredge[-] {x4} {f3} {x2, x5}; %
%           \factoredge[-] {x3} {f4} {x2, x6}; %
%         }
%     \end{subfigure}
% \end{figure*}
%
% % BEGIN COMMENT SECTION
%
% % \section{Baseline Models}
% % In order to facilitate comparisons with more complex models, it is prudent to
% % explore their static versions, models that encode naive independence assumptions
% % amongst the features $\mathbf{x}$ and class labels $y$. As done in Ng and Jordan
% % (2002) \cite{NgJordan}, the basic generative-discriminative pair is Naive Bayes
% % and Logistic Regression.
% %
% % \subsection{Generative: Naive Bayes}
% % The generative Naive Bayes classifier is a simple Bayesian network: it assumes
% % each feature $x_i \in \mathbf{x}$
% % is independent of one another, with class label $y$ as the sole parent node for all $x_i$.
% % Hence, the model factorizes as follows:
% % \begin{equation}
% %   \begin{split}
% %     p(y\, | \, x_1, \dots, x_n) \;\propto& \;\; p(y) \, \prod_{i=1}^n p(x_i \,|\, y) \\
% %   \end{split}
% % \end{equation}
% %
% % \subsection{Discriminative: Logistic Regression}
% % The discriminative analog to Naive Bayes is Logistic Regression, and can be
% % written as a log-linear model:
% % \begin{equation}
% %   \begin{split}
% %     p(y\,|\,\mathbf{x}; w) \;=&\; \frac{1}{Z} \phi \left( \mathbf{x}, \, y \right) \\
% %     Z \;=&\; \sum_y \phi(\mathbf{x}, \, y) \\
% %     \phi \left( \mathbf{x}, y \right) \;=&\; \exp \left( w^T f \left( \mathbf{x}, \, y \right) \right) \\
% %   \end{split}
% % \end{equation}
% % where $w$ is a parameter vector, $f(\mathbf{x}, \, y)$ is a feature extractor,
% % and $\phi(\mathbf{x}, \, y)$ is a the potential function.
% %
% % \section{General Models}
% % The baseline models are naive classifiers, and when discussing more complex
% % interactions between variables, I can use their general formulations.
% % \subsection{Generative: Bayesian Network}
% % The standard Bayesian network allows for feature nodes $x_i \in \mathbf{x}$ to
% % not be independent, as long as there are no cyclic structures. The factorization
% % is:
% % \begin{equation}
% %   p(y\, | \, x_1, \dots, x_n) \;\propto \;\; p(y) \, \prod_{i=1}^n p(x_i \,|\, pa(x_i) \cup y)
% % \end{equation}
% % where $pa(x_i)$ are the parents of node $x_i$ with respect to the current graph.
% % For images, I can model the pixels such that $pa(x_i)$ is equivalent to
% % the left, top, and top-left diagonal pixels in its neighborhood.
% % \subsection{Discriminative: Conditional Random Fields}
% % Conditional Random Fields (CRFs) are the general case for discriminative graphical models.
% % CRFs model conditional distributions $p(y\,|\,\mathbf{x})$ and takes on the form:
% % \begin{equation}
% %   \begin{split}
% %     p(y\,|\,\mathbf{x}) \;=&\; \frac{1}{Z} \, \prod_{\mathbf{c} \in \mathcal{C}} \phi_{\mathbf{c}} \left( \mathbf{x}, \, y_{\mathbf{c}} \right) \\
% %     Z \;=&\; \sum_{y} \prod_{\mathbf{c} \in \mathcal{C}} \phi_{\mathbf{c}} \left( \mathbf{x}, \, y_{\mathbf{c}} \right)
% %   \end{split}
% % \end{equation}
%
% % END SECTION
%
% \section{Parameter Estimation}
% Parameter estimation for generative models will use Maximum Likelihood Estimation.
% Logistic Regression will optimize its parameters via the L-BFGS algorithm.
% CRFs will use an approach depending on the library used (Section \ref{software}).
%
% \section{Inference}
% Inference for generative models will use belief propagation, if tractable.
% Otherwise, variational or sampling methods \cite{NUTS} will be
% used to approximate the conditional posterior.
%
% CRF inference will use loopy belief propagation or sampling techniques,
% since exact inference is intractable in general graphs.
% However, since Logistic Regression is a tree, message passing algorithms
% yield exact solutions.
%
% \section{Software}
% \label{software}
% This project will use OpenCV \cite{opencv} for image processing.
% Our generative models will use pgmpy\footnote{http://pgmpy.org} for learning
% and inference. Logistic Regression can be learned with scitkit-learn \cite{scikit-learn}.
% CRFs will either use PyStruct \cite{pystruct} or R's CRF library\footnote{https://cran.r-project.org/web/packages/CRF/index.html},
% depending on which library is easier to integrate with the preferred structure of
% the task.
%
% % \section{Additional Ideas}
% % If the aforementioned outline is tractable, then I could also look into
% % structured learning for Bayesian networks, running the PC algorithm to find
% % a good representation of the image network.
%
% % Might have to nix formulas and shorten descriptions

% ------------------------------



\bibliographystyle{abbrv}
\bibliography{report}

\end{document}
